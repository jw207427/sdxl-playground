{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8401dc3-5c93-4302-af92-67a7acc904c3",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5b78e-334d-43b2-b7ad-5dc6a48376d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "export PIP_ROOT_USER_ACTION=ignore\n",
    "\n",
    "pip install -Uq pip\n",
    "pip install autotrain-advanced==0.6.58\n",
    "pip install diffusers==0.21.4\n",
    "pip install autocrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5dc0ec-173b-444e-8212-18b38a85794e",
   "metadata": {},
   "source": [
    "### > check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037a11f-37a3-4805-be07-307d7055e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__) # e.g., 2.0.0 at time of post\n",
    "\n",
    "print(torch.cuda.get_device_name(0)) # e.g., NVIDIA A10G\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "assert device_count > 0, \"No GPU devices detected.\"\n",
    "\n",
    "print(\"Number of available GPU devices:\", device_count)\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec88ef2-34ea-4249-b1a1-6121224b7e5d",
   "metadata": {},
   "source": [
    "### > Prepare the images. The picture needs to be 1024 x 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10f6f4-9073-4abd-a844-3fbf03bb4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import utils\n",
    "import shutil\n",
    "\n",
    "imag_dir=Path(\"data\")\n",
    "dest_dir = Path(\"cropped\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for n,img_path in enumerate(chain(imag_dir.glob(\"*.[jJ][pP]*[Gg]\"),imag_dir.glob(\"*.[Pp][Nn][Gg]\"))):\n",
    "    try:\n",
    "        cropped = utils.resize_and_center_crop(img_path.as_posix(), 1024)\n",
    "        cropped.save(dest_dir / f\"image_{n}.png\")\n",
    "    except ValueError:\n",
    "        print(f\"Could not detect face in {img_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "print(\"Here are the preprocessed images ==========\")\n",
    "[x.as_posix() for x in dest_dir.iterdir() if x.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fd25a-babd-406a-8518-17c38a7b463c",
   "metadata": {},
   "source": [
    "- 8bit adam gobbles the images\n",
    "- prior-preservation exceeds A10G GPU memory\n",
    "- xformers gives package error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d5549-6d7d-48c0-a93f-71585aee80ce",
   "metadata": {},
   "source": [
    "### > Initialize fine tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd2295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713122b8-e3c7-4ba5-9e50-a59e07deb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project configuration\n",
    "project_name = \"finetune_jingswu\"\n",
    "\n",
    "model_name_base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "model_name_refiner = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "model_name_upscaler_4x = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
    "model_name_latent_upscaler = \"stabilityai/sd-x2-latent-upscaler\"\n",
    "\n",
    "# fine-tuning prompts\n",
    "# 'oue' is the rare tokens, 'car' is the class\n",
    "instance_prompt = \"photo of <<TOK>>\"\n",
    "class_prompt = \"photo of a person\"\n",
    "\n",
    "# fine-tuning hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_steps = 1000\n",
    "batch_size = 1\n",
    "gradient_accumulation = 4\n",
    "resolution = 1024\n",
    "num_class_image = 50\n",
    "\n",
    "class_image_path=Path(f\"/tmp/priors\")\n",
    "\n",
    "# environment variables for autotrain command\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name_base\n",
    "os.environ[\"INSTANCE_PROMPT\"] = instance_prompt\n",
    "os.environ[\"CLASS_PROMPT\"] = class_prompt\n",
    "os.environ[\"IMAGE_PATH\"] = dest_dir.as_posix()\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_STEPS\"] = str(num_steps)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"RESOLUTION\"] = str(resolution)\n",
    "os.environ[\"CLASS_IMAGE_PATH\"] = class_image_path.as_posix()\n",
    "os.environ[\"NUM_CLASS_IMAGE\"] = str(num_class_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb30a1-2c75-47db-96d3-7a229ac3bbde",
   "metadata": {},
   "source": [
    "### > use autotrain to fine tune\n",
    "\n",
    "help command will show all the available parameters\n",
    "\n",
    "```\n",
    "!autotrain dreambooth --help\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af966bb4-0f0b-4245-a789-44cd51b41ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!autotrain dreambooth \\\n",
    "    --model ${MODEL_NAME} \\\n",
    "    --project-name ${PROJECT_NAME} \\\n",
    "    --image-path \"${IMAGE_PATH}\" \\\n",
    "    --prompt \"${INSTANCE_PROMPT}\" \\\n",
    "    --class-prompt \"${CLASS_PROMPT}\" \\\n",
    "    --resolution ${RESOLUTION} \\\n",
    "    --batch-size ${BATCH_SIZE} \\\n",
    "    --num-steps ${NUM_STEPS} \\\n",
    "    --gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "    --lr ${LEARNING_RATE} \\\n",
    "    --fp16 \\\n",
    "    --gradient-checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baacc39-7596-472a-a5e3-c9b86f245fe2",
   "metadata": {},
   "source": [
    "### > Load the fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905520e-ba56-45ce-a4e8-eda6a1c4a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_name_base,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "pipeline.load_lora_weights(\n",
    "    project_name, \n",
    "    weight_name=\"pytorch_lora_weights.safetensors\",\n",
    "    adapter_name=\"jingswu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79947c0-b54d-46ce-b747-c1ffd55f0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_prompt = \"\"\"photo of <<TOK>>, epic front portrait, with glasses, zoom in, suit and tie, young and handsome, symmetry, blured background, futuristic cityscape\"\"\"\n",
    "\n",
    "subject_negative_prompt = \"\"\"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb454da2-ecaf-44df-b529-eb682a837aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = random.randint(0, 100000)\n",
    "generator = torch.Generator(device).manual_seed(seed)\n",
    "base_image = pipeline(\n",
    "    prompt=subject_prompt, \n",
    "    negative_prompt=subject_negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    output_type=\"pil\",\n",
    ").images[0]\n",
    "base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856a86b",
   "metadata": {},
   "source": [
    "## Load the weights file into model package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf793b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "s3_prefix = (\n",
    "    \"stable-diffusion-dreambooth-workshop\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "mme_prefix = f\"{s3_prefix}/inference/models\"\n",
    "\n",
    "model_data_url = f\"s3://{bucket}/{mme_prefix}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad3d50-3967-4894-93c6-8bba17f8ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = \"model_dir\"\n",
    "\n",
    "model_name = \"jingswu\"\n",
    "lora_weights = f\"{project_name}/pytorch_lora_weights.safetensors\"\n",
    "\n",
    "dest_dir = f\"{model_repo}/{model_name}/1\"\n",
    "\n",
    "shutil.copy(lora_weights, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebc705",
   "metadata": {},
   "source": [
    "Create the tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03440086",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97168df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_name = f\"{model_name}.tar.gz\"\n",
    "!tar -C $model_repo -czvf $tar_name $model_name\n",
    "\n",
    "sess.upload_data(path=tar_name, bucket=bucket, key_prefix=mme_prefix)\n",
    "!rm $tar_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b36313",
   "metadata": {},
   "source": [
    "## Deploy endpoint\n",
    "Now, you get the customer container url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r extended_triton_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c8800",
   "metadata": {},
   "source": [
    "you are now ready to configure and deploy the multi-model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "container = {\n",
    "    \"Image\": extended_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url, #f\"{model_data_url}{tar_name}\",     # S3 location of the models\n",
    "#     \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"jingswu\"}\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51cca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = name_from_base(f\"{mme_prefix.split('/')[0]}-models\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7753d",
   "metadata": {},
   "source": [
    "Create a SageMaker endpoint configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6596b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = name_from_base(f\"{mme_prefix.split('/')[0]}-epc\")\n",
    "\n",
    "instance_type = 'ml.g5.2xlarge'\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb8b69",
   "metadata": {},
   "source": [
    "Create the endpoint, and wait for it to transition to InService state. (This takes about 5 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(f\"{mme_prefix.split('/')[0]}-ep\")\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f78b12",
   "metadata": {},
   "source": [
    "## Invoke the LoRA fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2296f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to encode and decode images\n",
    "def decode_image(img):\n",
    "    buff = BytesIO(base64.b64decode(img.encode(\"utf8\")))\n",
    "    image = Image.open(buff)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "prompt = \"\"\"photo of <<TOK>>, epic front portrait, with glasses, zoom in, suit and tie, young and handsome, symmetry, blured background, futuristic cityscape\"\"\"\n",
    "\n",
    "negative_prompt = \"\"\"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, \n",
    "watermark, grainy, signature, cut off, draft, amateur, multiple, gross, weird, uneven, furnishing, decorating, decoration, furniture, text, poor, low, basic, worst, juvenile, \n",
    "unprofessional, failure, crayon, oil, label, thousand hands\"\"\"\n",
    "\n",
    "seed = random.randint(1, 1000000000)\n",
    "gen_args = json.dumps(dict(num_inference_steps=50, guidance_scale=7, height=1024, width=1024, seed=seed))\n",
    "\n",
    "inputs = dict(prompt = prompt,\n",
    "              negative_prompt = negative_prompt,\n",
    "              gen_args = gen_args)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": [1,1], \"datatype\": \"BYTES\", \"data\": [data]} for name, data in inputs.items()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e348758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=tar_name,\n",
    ")\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "original_image = decode_image(output[0][\"data\"][0])\n",
    "original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497e1d5",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce75558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72100b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
